# -*- coding: utf-8 -*-
"""Suhaan/NickCoLabAndroidMalwareDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11YwUdLjVc3xHdA7LdDDIWfJv9f5Sc3Xe

# Reading in the Data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from imblearn.over_sampling import SMOTE
import imblearn

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

import statsmodels.api as sm

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Potato/TUANDROMD.csv")
data.head()

data = pd.read_csv("suhaan.csv")
data.head()

"""# Exploratory Data Analysis"""

# data.head
# data.tail
# data.info
# data.describe
# data["ACCESS_ALL_DOWNLOADS"].unique
# data["ACCESS_ALL_DOWNLOADS"].sort_values
# data["ACCESS_ALL_DOWNLOADS"].value_counts
# data["ACCESS_ALL_DOWNLOADS"].nlargest
# data["ACCESS_ALL_DOWNLOADS"].nsmallest
data["ACCESS_ALL_DOWNLOADS"].size
data.mean
data.std
data.median

"""### Exploratory Data Analysis (EDA)"""

# Class Balancing
# How many malware observations do we have vs. how many not malware?
# Another way to think about it, how many instances of malware do we have? How many instances of not malware?

data.shape
# data.head

data["ACCESS_ALL_DOWNLOADS"].value_counts()

data["ACCESS_WIFI_STATE"].value_counts()

data["ACCESS_NETWORK_STATE"].value_counts()

data["Label"].value_counts()

data["ACCESS_NETWORK_STATE"].value_counts().plot(kind='bar', title='Malware vs. Not Malware')
plt.xlabel('Malware')
plt.ylabel('Count')
plt.show()

filtered_data = data[data['ACCESS_ALL_DOWNLOADS'].isin(['ACCESS_WIFI_STATE'])]


# Calculate the correlation matrix
correlation_matrix = filtered_data.corr()

# Create the heatmap
plt.figure(figsize = (10,8))
sns.heatmap(correlation_matrix, cmap = 'coolwarm')
plt.show()

"""# Data Preprocessing

## Preprocessing Order:


1.   Deal with missing data
2.   Make dummy variables for our categorical variables
3.   Scale our numeric variables (MinMaxScaler: smushes values between 0 and 1)
4.   X and y split THEN Train/Test Split
5.   Balance our classes on our training data (X_train, y_train --> X_smote, y_smote)
6.   Feed preprocessed data into algorithm(s)

## Removing Missing Data
"""

# Check Our Data to See if We Have Any Missing Outcome Values
data["Label"].isnull().sum()

# Drop Missing Values from "y"
data2 = data.dropna(subset=["Label"])

"""## Dummy Variables and/or MinMax Scaling (if necessary)

## Train/Test Split
"""

# Create our X/y
X = data2.drop(["Label"], axis=1)
y = data2["Label"]

# Create the Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)

# Create SMOTE Object
smote = SMOTE()

# fit predictor and target variable
X_smote, y_smote = smote.fit_resample(X_train, y_train)

print('Original dataset shape', len(y_train))
print('Resample dataset shape', len(y_smote))

# from sklearn import preprocessing
# import matplotlib.pyplot as plt
# plt.rc("font", size=14)
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# import seaborn as sns
# sns.set(style="white")
# sns.set(style="whitegrid", color_codes=True)


# data['ACCESS_ALL_DOWNLOADS']=np.where(data['ACCESS_ALL_DOWNLOADS'] =='basic.9y', 'Basic', data['ACCESS_ALL_DOWNLOADS'])
# data['ACCESS_ALL_DOWNLOADS']=np.where(data['ACCESS_ALL_DOWNLOADS'] =='basic.6y', 'Basic', data['ACCESS_ALL_DOWNLOADS'])
# data['ACCESS_ALL_DOWNLOADS']=np.where(data['ACCESS_ALL_DOWNLOADS'] =='basic.4y', 'Basic', data['ACCESS_ALL_DOWNLOADS'])

"""## Logistic Regression"""

# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)
logreg = LogisticRegression() # Create the model
logreg.fit(X_smote, y_smote) # Fitting the model

y_pred = logreg.predict(X_test) # getting predictions
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test))) # getting accuracy

# Confusion Matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

# Classification Report
print(classification_report(y_test, y_pred))

# Statsmodel (Suhaan to Review for Homework and Learn More About Coefficient Interpretation)
# !pip install statsmodels
result = sm.Logit(y_smote, X_smote)
print(result.summary())

# DATA ISSUE: LIKELY NEED TO MAP y to 0/1

"""New models currently being worked on


"""

# Printing Accuracy from test and predict (does not work)

# !pip install sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score



scaler = StandardScaler()
X_smote = scaler.fit_transform(X_smote)
X_test = scaler.transform(X_test)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_smote, y_smote)
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""Accuracy Score"""

# Error
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_score
k_values = [i for i in range (1,31)]
scores = []

scaler = StandardScaler()
X = scaler.fit_transform(X)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X, y, cv=5)
    scores.append(np.mean(score))
sns.lineplot(x = k_values, y = scores, marker = 'o')
plt.xlabel("K Values")
plt.ylabel("Accuracy Score")

"""Accuracy, Recall, Precision"""

# Error
from sklearn.metrics import recall_score

best_index = np.argmax(scores)
best_k = k_values[best_index]

knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, pos_label='malware')
recall = recall_score(y_test, y_pred, pos_label='malware')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

"""Confusion Matrix"""

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

"""Classification report from y test and pred"""

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

"""# Random Forest

COLAB AI CREATED:
"""

# ERROR (SAME AS MINE)


# Import necessary modules
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint

# Split the data into features (X) and target (y)
X = data.drop('ACCESS_ALL_DOWNLOADS', axis=1)
y = data['ACCESS_ALL_DOWNLOADS']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Check for non-numerical values in X_train and y_train
object_cols = X_train.select_dtypes(include='object').columns

# If non-numerical values are present, handle them (e.g., drop rows or convert to numerical)
if len(object_cols) > 0:
    # Option 1: Drop rows with non-numerical values
    X_train = X_train.dropna(subset=object_cols)
    y_train = y_train.dropna()

    # Option 2: Convert non-numerical values to numerical ones
    # (e.g., using LabelEncoder or OneHotEncoder)
    # from sklearn.preprocessing import LabelEncoder
    # label_encoder = LabelEncoder()
    # for col in object_cols:
    #     X_train[col] = label_encoder.fit_transform(X_train[col])

# Create a Random Forest classifier
rf = RandomForestClassifier()

# Fit the model
rf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Print the precision and recall scores
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
print("Precision:", precision)
print("Recall:", recall)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

# ERROR (ValueError: could not convert string to float: 'malware')


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint

# Tree Visualisation
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

data['ACCESS_ALL_DOWNLOADS'].fillna(data['ACCESS_ALL_DOWNLOADS'].mean(), inplace=True)

# Split the data into features (X) and target (y)
X = data.drop('ACCESS_ALL_DOWNLOADS', axis=1)
y = data['ACCESS_ALL_DOWNLOADS']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create a Random Forest classifier
rf = RandomForestClassifier()

# Fit the model
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Export the first three decision trees from the forest

for i in range(3):
    tree = rf.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=X_train.columns,
                               filled=True,
                               max_depth=2,
                               impurity=False,
                               proportion=True)
    graph = graphviz.Source(dot_data)
    display(graph)

"""# Hyperparameter Tuning

"""

param_dist = {'n_estimators': randint(50,500),
              'max_depth': randint(1,20)}

# Create a random forest classifier
rf = RandomForestClassifier()

# Use random search to find the best hyperparameters
rand_search = RandomizedSearchCV(rf,
                                 param_distributions = param_dist,
                                 n_iter=5,
                                 cv=5)

# Fit the random search object to the data
rand_search.fit(X_train, y_train)

# Create a variable for the best model
best_rf = rand_search.best_estimator_

# Print the best hyperparameters
print('Best hyperparameters:',  rand_search.best_params_)

# Generate predictions with the best model
y_pred = best_rf.predict(X_test)

# Create the confusion matrix
cm = confusion_matrix(y_test, y_pred)

ConfusionMatrixDisplay(confusion_matrix=cm).plot();